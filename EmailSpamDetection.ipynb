{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Coursework-Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHGQ78mTkeqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "95eb7ebe-e90b-4f5b-cc33-2969628c97e6"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-hhNOS0keqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9f2482ed-9013-4ce1-9c38-623818833c09"
      },
      "source": [
        "%cd\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz > /dev/null\n",
        "!pip install -q findspark\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/root/spark-2.4.5-bin-hadoop2.7\"\n",
        "%cd /content\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "# get a spark context\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "print(sc)\n",
        "# get the context\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "print(spark) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "<pyspark.sql.session.SparkSession object at 0x7fa093408b00>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nncrHFdwqUmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "b49286a7-b7c6-4fa2-a3e5-d8dac5b3c910"
      },
      "source": [
        "%cd /content/drive/My Drive/BigData2020/data/lingspam_public \n",
        "\n",
        "!cat readme.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BigData2020/data/lingspam_public\n",
            "This directory contains the Ling-Spam corpus, as described in the \n",
            "paper:\n",
            "\n",
            "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \n",
            "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \n",
            "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \n",
            "Proceedings of the Workshop on Machine Learning in the New Information \n",
            "Age, 11th European Conference on Machine Learning (ECML 2000), \n",
            "Barcelona, Spain, pp. 9-17, 2000.\n",
            "\n",
            "There are four subdirectories, corresponding to four versions of \n",
            "the corpus:\n",
            "\n",
            "bare: Lemmatiser disabled, stop-list disabled.\n",
            "lemm: Lemmatiser enabled, stop-list disabled.\n",
            "lemm_stop: Lemmatiser enabled, stop-list enabled.\n",
            "stop: Lemmatiser disabled, stop-list enabled.\n",
            "\n",
            "Each one of these 4 directories contains 10 subdirectories (part1, \n",
            "..., part10). These correspond to the 10 partitions of the corpus \n",
            "that were used in the 10-fold experiments. In each repetition, one \n",
            "part was reserved for testing and the other 9 were used for training. \n",
            "\n",
            "Each one of the 10 subdirectories contains both spam and legitimate \n",
            "messages, one message in each file. Files whose names have the form\n",
            "spmsg*.txt are spam messages. All other files are legitimate messages.\n",
            "\n",
            "By obtaining a copy of this corpus you agree to acknowledge the use \n",
            "and origin of the corpus in any published work of yours that makes \n",
            "use of the corpus, and to notify the person below about this work.\n",
            "\n",
            "Ion Androutsopoulos \n",
            "http://www.aueb.gr/users/ion/\n",
            "Ling-Spam corpus last updated: July 17, 2000\n",
            "This file (readme.txt) last updated: July 30, 2003.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb02XcMOkeqq",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset and create RDDs \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7iF1lZukeqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "2017d30f-06e1-41dc-e478-b98d5e4bb9df"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def makeTestTrainRDDs(pathString):\n",
        "    \"\"\" Takes one of the four subdirectories of the lingspam dataset and returns two RDDs one each for testing and training. \"\"\"\n",
        "    \n",
        "    p = Path(pathString) # gets a path object representing the current directory path.\n",
        "    dirs = list(p.iterdir()) # get the directories part1 ... part10. \n",
        "    print(dirs) # Print to check that you have the right directory. You can comment this out when checked. \n",
        "    rddList = [] # create a list for the RDDs\n",
        "    # now create an RDD for each 'part' directory and add them to rddList\n",
        "    print('creating RDDs')\n",
        "    for d in dirs: # iterate through the directories\n",
        "        dir_path = str(d.resolve())\n",
        "        print(dir_path)\n",
        "        rdd = sc.wholeTextFiles(dir_path) #>>> # read the files in the directory \n",
        "        rddList.append(rdd) #>>> append the RDD to the rddList\n",
        "    print('len(rddList)', len(rddList))  # we should now have 10 RDDs in the list # just for testing\n",
        "    print(rddList[1].take(1)) # just for testing, comment out when it works.\n",
        "\n",
        "    testRDD1 = rddList[9] # set the test set\n",
        "    trainRDD1 = rddList[0] # start the training set from 0 and \n",
        "    # now loop over the range from 1 to 9 (exclusive) to create a union of the remaining RDDs\n",
        "    print('creating RDD union')\n",
        "    for i in range(1, 9):\n",
        "        trainRDD1 = trainRDD1.union(rddList[i]) #>>> create a union of the current and the next \n",
        "            # RDD in the list, so that in the end have a union of all parts 0-8. (9 is used as test set)\n",
        "    # both RDDs should remove the paths and extensions from the filename. \n",
        "    testRDD2 = testRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1])) \n",
        "    trainRDD2 = trainRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1])) \n",
        "    return (trainRDD2, testRDD2)\n",
        "\n",
        "\n",
        "%cd /content/drive/My Drive/BigData2020/data/lingspam_public \n",
        "\n",
        "!ls \n",
        "# the code below is for testing the function makeTestTrainRDDs\n",
        "trainRDD_testRDD = makeTestTrainRDDs('bare') # read from the directory \n",
        "(trainRDD, testRDD) = trainRDD_testRDD # unpack the returned tuple\n",
        "print('created the RDDs') # notify the user, so that be able to figure out where things went wrong if they do.\n",
        "print('testRDD.count(): ', testRDD.count()) # should be ~289\n",
        "#print('trainRDD.count(): ', trainRDD.count()) # should be ~2604 - commented out to save time as it takes some time to create RDD from all the files\n",
        "print('testRDD.getNumPartitions():', testRDD.getNumPartitions()) # normally 2 on Colab (single machine)\n",
        "print('testRDD.getStorageLevel():', testRDD.getStorageLevel()) # Serialized, 1x Replicated, expected to be (False, False, False, False, 1) \n",
        "print('testRDD.take(1): ', testRDD.take(1)) # should be (filename, message) \n",
        "rdd1 = testRDD # use this for development in the next tasks "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BigData2020/data/lingspam_public\n",
            "bare  lemm  lemm_stop  readme.txt  stop\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "created the RDDs\n",
            "testRDD.count():  289\n",
            "testRDD.getNumPartitions(): 2\n",
            "testRDD.getStorageLevel(): Serialized 1x Replicated\n",
            "testRDD.take(1):  [('3-1msg1', 'Subject: re : 2 . 882 s - > np np\\n\\n> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that \\' john mcnamara the name \\' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were \\' chaim shmendrik \\' , \\' john mcnamara the name \\' would be false . no tautology , this . ( and no reduplication , either . )\\n')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUKltxq5keqw",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize and remove punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AB_nfmhYkeqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f9056ba6-a049-4309-8548-fcc126846d3f"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\" Apply the nltk.word_tokenize() method to our text, return the token list. \"\"\"\n",
        "    nltk.download('punkt') #  loads the standard NLTK tokenizer model \n",
        "    # it is important that this is done here in the function, as it needs to be done on every worker.\n",
        "    # If do the download outside this function, it would only be executed on the driver     \n",
        "    return (nltk.word_tokenize(text)) # use the nltk function word_tokenize\n",
        "    \n",
        "def removePunctuation(tokens):\n",
        "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
        "   \n",
        "    tokens2 =  [re.sub('[()\\[\\],.?!\";_]','', s) for s in tokens] # used a list comprehension to remove punctuaton\n",
        "    return tokens2\n",
        "    \n",
        "def prepareTokenRDD(fn_txt_RDD):\n",
        "    \"\"\" Take an RDD with (filename, text) elements and transform it into a (filename, [token ...]) RDD without punctuation characters. \"\"\"\n",
        "    rdd_vals2 = fn_txt_RDD.values() # It's convenient to process only the values. \n",
        "    rdd_vals3 = rdd_vals2.map(tokenize) # Create a tokenised version of the values by mapping\n",
        "    rdd_vals4 = rdd_vals3.map(removePunctuation) # remove punctuation from the values\n",
        "    rdd_kv = fn_txt_RDD.keys().zip(rdd_vals4) # zip the two RDDs together \n",
        "    # i.e. produce tuples with one item from each RDD.\n",
        "    # This works because have only applied mappings to the values, \n",
        "    # therefore the items in both RDDs are still aligned.\n",
        "    # now remove any empty strings (i.e. length 0) \n",
        "    # created by removing punctuation, and resulting entries without words left.\n",
        "    rdd_kvr = rdd_kv.map(lambda v: (v[0], [e for e in v[1] if len(e)>0])) # remove empty strings from token lists using RDD.map and a lambda. TIP len(s) gives you the lenght of string. \n",
        "    rdd_kvrf = rdd_kvr.filter(lambda v: [len(v[1])>0]) # remove empty token lists using RDD.filter and a lambda. \n",
        "    \n",
        "    return rdd_kvrf \n",
        "\n",
        "rdd2 = prepareTokenRDD(rdd1) # Use a small RDD for testing.\n",
        "print(rdd2.take(1)) # For checking result of task 2. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3-1msg1', ['Subject', ':', 're', ':', '2', '882', 's', '-', '>', 'np', 'np', '>', 'date', ':', 'sun', '15', 'dec', '91', '02', ':', '25', ':', '02', 'est', '>', 'from', ':', 'michael', '<', 'mmorse', '@', 'vm1', 'yorku', 'ca', '>', '>', 'subject', ':', 're', ':', '2', '864', 'queries', '>', '>', 'wlodek', 'zadrozny', 'asks', 'if', 'there', 'is', '``', 'anything', 'interesting', '``', 'to', 'be', 'said', '>', 'about', 'the', 'construction', '``', 's', '>', 'np', 'np', '``', 'second', '>', 'and', 'very', 'much', 'related', ':', 'might', 'we', 'consider', 'the', 'construction', 'to', 'be', 'a', 'form', '>', 'of', 'what', 'has', 'been', 'discussed', 'on', 'this', 'list', 'of', 'late', 'as', 'reduplication', 'the', '>', 'logical', 'sense', 'of', '``', 'john', 'mcnamara', 'the', 'name', '``', 'is', 'tautologous', 'and', 'thus', 'at', '>', 'that', 'level', 'indistinguishable', 'from', '``', 'well', 'well', 'now', 'what', 'have', 'we', 'here', '``', 'to', 'say', 'that', \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'is', 'tautologous', 'is', 'to', 'give', 'support', 'to', 'those', 'who', 'say', 'that', 'a', 'logic-based', 'semantics', 'is', 'irrelevant', 'to', 'natural', 'language', 'in', 'what', 'sense', 'is', 'it', 'tautologous', 'it', 'supplies', 'the', 'value', 'of', 'an', 'attribute', 'followed', 'by', 'the', 'attribute', 'of', 'which', 'it', 'is', 'the', 'value', 'if', 'in', 'fact', 'the', 'value', 'of', 'the', 'name-attribute', 'for', 'the', 'relevant', 'entity', 'were', \"'\", 'chaim', 'shmendrik', \"'\", \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'would', 'be', 'false', 'no', 'tautology', 'this', 'and', 'no', 'reduplication', 'either'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9HgRAEGkeq0",
        "colab_type": "text"
      },
      "source": [
        "##  Creating normalised TF.IDF vectors of defined dimensionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_YQKAokeq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a35d3b5a-641d-4509-eada-4e02af02d2cd"
      },
      "source": [
        "# use the hashing trick to create a fixed-size vector from a word list\n",
        "def hashing_vectorize(text, N): # arguments: the list and the size of the output vector\n",
        "    v = [0] * N  # create vector of 0s\n",
        "    for word in text: # iterate through the words \n",
        "      h = hash(word)  # get the hash value \n",
        "      v[h % N] = v[h % N] + 1  # add 1 at the hashed address\n",
        "    return v # return hashed word vector\n",
        "\n",
        "from pyspark.mllib.feature import IDF, Normalizer\n",
        "\n",
        "def normTFIDF(fn_tokens_RDD, vecDim):\n",
        "    keysRDD = fn_tokens_RDD.keys()\n",
        "    tokensRDD = fn_tokens_RDD.values()\n",
        "    tfVecRDD = tokensRDD.map(lambda tokens: hashing_vectorize(tokens, vecDim)) \n",
        "    idf = IDF() # create IDF object\n",
        "    idfModel = idf.fit(tfVecRDD) # calculate IDF values\n",
        "    tfIdfRDD = idfModel.transform(tfVecRDD) # 2nd pass needed (see lecture slides), transforms RDD\n",
        "    norm = Normalizer(1) # create a Normalizer object like in the example linked above\n",
        "    normTfIdfRDD = norm.transform(tfIdfRDD) # and apply it to the tfIdfRDD \n",
        "    zippedRDD = keysRDD.zip(normTfIdfRDD) # zip the keys and values together\n",
        "    return zippedRDD\n",
        "\n",
        "testDim = 10 # too small for good accuracy, but OK for testing\n",
        "rdd3 = normTFIDF(rdd2, testDim) # test normTFIDF function\n",
        "print(rdd3.take(1)) # now have tuples with ('filename', [N-dim vector])\n",
        "# e.g. [('3-1msg1', DenseVector([0.0, 0.1629, 0.6826, 0.0, 0.0, 0.0, 0.4017, 0.3258, 0.3133, 0.3766]))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3-1msg1', DenseVector([0.0, 0.072, 0.3017, 0.0, 0.0, 0.0, 0.1775, 0.144, 0.1384, 0.1664]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xzB6hrkeq5",
        "colab_type": "text"
      },
      "source": [
        "## Create LabeledPoints \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pixiedust": {
          "displayParams": {
            "handlerId": "tableView"
          }
        },
        "id": "wF9BDmnEkeq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b6708ae-ac4e-46e2-a124-aadbc2fb272a"
      },
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "# create labelled points of vector size N out of an RDD with normalised (filename, td.idf-vector) items\n",
        "def makeLabeledPoints(fn_vec_RDD): # RDD and N needed \n",
        "    # determine the true class as encoded in the filename and represent as 1 (spam) or 0 (good) \n",
        "    cls_vec_RDD = fn_vec_RDD.map(lambda v: (1 if v[0].startswith(\"spmsg\") else 0, v[1])) # use a conditional expression to get the class label (True or False)\n",
        "    # now create the LabeledPoint objects with (class, vector) arguments\n",
        "    lp_RDD = cls_vec_RDD.map(lambda cls_vec: LabeledPoint(cls_vec[0], cls_vec[1]) ) \n",
        "    return lp_RDD \n",
        "\n",
        "# for testing\n",
        "testLpRDD = makeLabeledPoints(rdd3)\n",
        "print(testLpRDD.take(1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LabeledPoint(0.0, [0.0,0.07199034703813956,0.3016523635774738,0.0,0.0,0.0,0.17751412657278054,0.14398069407627911,0.1384429750733453,0.16641949366198175])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuLuz17zkeq8",
        "colab_type": "text"
      },
      "source": [
        "##  Complete the preprocessing \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9F8Kl3ukeq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a8438135-9a6b-4f41-a1f6-de182862416e"
      },
      "source": [
        "# now apply the preprocessing chain to the data loaded \n",
        "# N is for controlling the vector size\n",
        "def preprocess(rawRDD, N):\n",
        "    \"\"\" take a (filename,text) RDD and transform into LabelledPoint objects \n",
        "        with class labels and a TF.IDF vector with N dimensions. \n",
        "    \"\"\"\n",
        "    a = prepareTokenRDD(rawRDD)\n",
        "    b = normTFIDF(a , N)\n",
        "    c = makeLabeledPoints(b)\n",
        "    return c # apply tasks 2, 3 and 4 here\n",
        "    \n",
        "\n",
        "# and with this we can start the whole process from a directory, N is again the vector size\n",
        "def loadAndPreprocess(directory, N):\n",
        "    \"\"\" load lingspam data from a directory and create a training and test set of preprocessed data \"\"\"\n",
        "    trainRDD_testRDD = makeTestTrainRDDs(directory) # read from the directory using the function created in task 1\n",
        "    (trainRDD, testRDD) =  trainRDD_testRDD# unpack the returned tuple\n",
        "    return (preprocess(trainRDD, N), preprocess(testRDD, N)) # apply the preprocessing function defined above\n",
        "\n",
        "trainLpRDD = preprocess(trainRDD, testDim) # prepare the training data\n",
        "print(trainLpRDD.take(1)) # should look similar to previous cell's output\n",
        "\n",
        "train_test_LpRDD = loadAndPreprocess('lemm', 100) # re-run with another vector size\n",
        "(trainLpRDD, testLpRDD) = train_test_LpRDD\n",
        "print(testLpRDD.take(1))\n",
        "print(trainLpRDD.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LabeledPoint(0.0, [0.20078443042124564,0.144815703802104,0.13139074894555822,0.0,0.0,0.07041005093669801,0.13403686030341527,0.13282429900512271,0.10525380815431767,0.08048409843153845])]\n",
            "[PosixPath('lemm/part10'), PosixPath('lemm/part9'), PosixPath('lemm/part8'), PosixPath('lemm/part7'), PosixPath('lemm/part6'), PosixPath('lemm/part5'), PosixPath('lemm/part4'), PosixPath('lemm/part3'), PosixPath('lemm/part2'), PosixPath('lemm/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/lemm/part9/8-829msg1.txt', \"Subject: re : 8 . 810 , sum : french loan word , language evolution\\n\\njust see your summary on french loanword in english . i think baugh take the 10 , 0 word figure from jespersen 's _ growth and structure of the english language _ . it would be more like jespersen than baugh to actually try to count them , in fact . it 's worth look at jespersen for all sort of fact like these , in his chapter on the french influence in that book . - - suzanne kemmer\\n\")]\n",
            "creating RDD union\n",
            "[LabeledPoint(0.0, [0.006611352786509284,0.011030504275905104,0.014988869362605904,0.008805028445489612,0.012754561365349247,0.028773732253765855,0.0,0.033261448818746556,0.01129292424708123,0.021639878576396175,0.0,0.010714323559394587,0.0,0.021899776368830243,0.01113449785802156,0.02101678178878669,0.0,0.0029863100426968056,0.009591244084588618,0.022491724327070983,0.0084072387420903,0.0049625319939126476,0.0,0.0,0.0,0.013425854399669514,0.018359797555145718,0.011798969409780076,0.0,0.0,0.0,0.0,0.0,0.0,0.0191587716083176,0.0,0.028322601307955823,0.0,0.016818370800356173,0.0,0.0,0.0490400874026791,0.008552905914440842,0.02753352777708347,0.0,0.0,0.014284916343562989,0.019182488169177236,0.017387887807773766,0.01242403508776178,0.008262220423437447,0.011934453550508685,0.004894523304030447,0.051684365762004546,0.0,0.008993859274307557,0.008552905914440842,0.0,0.0,0.015346637344324343,0.0,0.0,0.0,0.0,0.04390735467991102,0.008437157290735737,0.0,0.0,0.0,0.03716472604631782,0.0038065564066604857,0.057139665374251955,0.0,0.013236605131086127,0.02452004370133955,0.012754561365349247,0.03293051600993327,0.0,0.0,0.018163080589831702,0.010430935297758336,0.0,0.0,0.0,0.01858236302315891,0.0,0.005756633769507724,0.012588878775163301,0.0,0.013451110864658064,0.0,0.020818063621227987,0.01936161686705345,0.010508390894393345,0.0,0.008119832237662702,0.0,0.0,0.0,0.0])]\n",
            "[LabeledPoint(0.0, [0.005614744500826249,0.003260967103879289,0.007740243659615174,0.005969085285029252,0.008104434790027407,0.011384121030423683,0.006751202666873336,0.0041397636095168965,0.005368513017544478,0.008872569483016169,0.0,0.04870363899880606,0.007429938799327332,0.007533243502806478,0.0027734340628843704,0.012297171352983017,0.0052283734250785615,0.002764720287968351,0.011254884178049868,0.05494603269606611,0.010616747706650392,0.00672293047851267,0.0037947070101412277,0.002733942263277317,0.0020629012180334804,0.03951649768448488,0.0513830842429,0.004970303790771186,0.0057129948927908385,0.012235092276707854,0.013497539579566363,0.0028037447027355455,0.008932538547626585,0.01595416351613405,0.005939030612551983,0.0024655204089721414,0.02107153045407126,0.009045524038103927,0.015071390502773417,0.0015236556442676792,0.016300730298624513,0.029518792759938906,0.011483133055605742,0.007552801786631381,0.001437343080994814,0.005764157922507598,0.015416397642675446,0.00900829544419846,0.023402443111056104,0.008377486206708327,0.010239606554480205,0.005990842478892577,0.002848957010919987,0.013306481921458257,0.005576339595604765,0.0072666566677848,0.016542697964338687,0.021474052070177913,0.006943003086795751,0.013021905542085275,0.03998326099748277,0.0029137994094536514,0.0018264384632948939,0.007407680899345233,0.01984299756447831,0.00649394689744206,0.008248172728375244,0.0024655204089721414,0.002024638036137022,0.009330458382350849,0.0051088181169138165,0.00879718447647909,0.00210138516957559,0.0031969867646748505,0.0,0.01784830690593148,0.004886844080626706,0.0039360371370232485,0.005230826468035248,0.011395831082952891,0.003645926884442922,0.0232664757821184,0.003425866142816766,0.0,0.015623782494384434,0.0021754768862896714,0.008423000125242731,0.019399069161451434,0.002619204055379623,0.005850472085150956,0.00911340615465848,0.0,0.01204390787787344,0.009393691821710445,0.0,0.006962644494415559,0.010192632683723559,0.006126009555750808,0.007943820578267964,0.0030924330035034067])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdwyXqu4keq_",
        "colab_type": "text"
      },
      "source": [
        "## Train some classifiers  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljcgl0mLkeq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "befc4f85-e42a-4aa7-a9fb-e8ef41ca9273"
      },
      "source": [
        "from pyspark.mllib.classification import NaiveBayes, LogisticRegressionWithLBFGS, SVMWithSGD\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# train the model with a LabeledPoint RDD.\n",
        "def trainModel(lpRDD):\n",
        "    \"\"\" Train 3 classifier models on the given RDD with LabeledPoint objects. A list of trained model is returned. \"\"\"\n",
        "    # Train a classifier model.\n",
        "    print('Starting to train the model') # give some immediate feedback\n",
        "    model1 = LogisticRegressionWithLBFGS.train(lpRDD) # this is the best model\n",
        "    print('Trained LR (model1)')\n",
        "    #print('type(model1)')\n",
        "    model2 = NaiveBayes.train(lpRDD) # doesn't work well\n",
        "    print('Trained NB (model2)')\n",
        "    #print(type(model2))\n",
        "    model3 = SVMWithSGD.train(lpRDD) # or this ...\n",
        "    print('Trained SVM (model3)')\n",
        "    return [model1, model2, model3]\n",
        "\n",
        "def testModel(model, lpRDD):\n",
        "    \"\"\" Tests the classification accuracy of the given model on the given RDD with LabeledPoint objects. \"\"\"\n",
        "    lpRDD.persist(StorageLevel.MEMORY_ONLY)\n",
        "    # Make prediction and evaluate training set accuracy\n",
        "    # Get the prediction and the ground truth label\n",
        "    predictionAndLabel = lpRDD.map(lambda p: (model.predict(p.features), p.label)) # get the prediction and ground truth (label) for each item\n",
        "    correct = predictionAndLabel.filter(lambda xv: xv[0] == xv[1]).count() # count the correct predictions \n",
        "    accuracy = correct/lpRDD.count() # and calculate the accuracy \n",
        "    print('Accuracy {:.1%} (data items: {}, correct: {})'.format(accuracy, lpRDD.count(), correct)) # report to console\n",
        "    return accuracy # and return the value  \n",
        "\n",
        "models = trainModel(trainLpRDD) # just for testing\n",
        "testModel(models[2], trainLpRDD) # just for testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Accuracy 83.4% (data items: 2604, correct: 2171)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8337173579109063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TeHFXcTkerC",
        "colab_type": "text"
      },
      "source": [
        "## Automate training and testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj_Xh5oekerD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function combines the previous two functions\n",
        "# this method should take RDDs with LabeledPoints\n",
        "def trainTestModel(trainRDD, testRDD):\n",
        "    \"\"\" Trains 3 models and tests them on training and test data. Returns a matrix with the training and testing (rows) accuracy values for all models (columns). \"\"\"\n",
        "    models = trainModel(trainRDD) # train models on the training set\n",
        "    results = [[], []] # matrix for 2 modes (training/test) vs n models (currently 3)\n",
        "    for mdl in models:\n",
        "        print('Training')\n",
        "        results[0].append(mdl) # test the model on the training set\n",
        "        print('Testing')\n",
        "        results[1].append(testModel(mdl , testRDD)) # test the model on the test set\n",
        "    return results\n",
        "\n",
        "def trainTestFolder(folder,N):\n",
        "    \"\"\" Reads data from a folder, preproceses the data, and trains and evaluates models on it. \"\"\"\n",
        "    print('Start loading and preprocessing') \n",
        "    train_test_LpRDD = loadAndPreprocess(folder,N) # create the RDDs\n",
        "    print('Finished loading and preprocessing')\n",
        "    (trainLpRDD, testLpRDD) = train_test_LpRDD # unpack the RDDs \n",
        "    return trainTestModel(trainLpRDD,testLpRDD) # train and test\n",
        "\n",
        "trainTestFolder('lemm', 1000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzcNex1NkerF",
        "colab_type": "text"
      },
      "source": [
        "## Run experiments \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HmW_CpetkerG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2d8cb18-6e9a-45b1-eed7-81e9a2f7dd01"
      },
      "source": [
        "folder = 'bare'\n",
        "vectorsizes = [3, 30, 300, 3000, 30000]\n",
        "print('EXPERIMENT 1: Testing different vector sizes')\n",
        "results_vectorsizes = []\n",
        "for n in vectorsizes:\n",
        "    print('\\nN = {}'.format(n))\n",
        "    result = {'n': n, 't': folder}\n",
        "    result['acc'] = trainTestFolder(folder, n)\n",
        "    results_vectorsizes.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXPERIMENT 1: Testing different vector sizes\n",
            "\n",
            "N = 3\n",
            "Start loading and preprocessing\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "\n",
            "N = 30\n",
            "Start loading and preprocessing\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 85.1% (data items: 289, correct: 246)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "\n",
            "N = 300\n",
            "Start loading and preprocessing\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 95.2% (data items: 289, correct: 275)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "\n",
            "N = 3000\n",
            "Start loading and preprocessing\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n",
            "Trained SVM (model3)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 98.6% (data items: 289, correct: 285)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "Training\n",
            "Testing\n",
            "Accuracy 83.4% (data items: 289, correct: 241)\n",
            "\n",
            "N = 30000\n",
            "Start loading and preprocessing\n",
            "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
            "creating RDDs\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part10\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part8\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part7\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part6\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part5\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part4\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part3\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part2\n",
            "/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part1\n",
            "len(rddList) 10\n",
            "[('file:/content/drive/My Drive/BigData2020/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
            "creating RDD union\n",
            "Finished loading and preprocessing\n",
            "Starting to train the model\n",
            "Trained LR (model1)\n",
            "Trained NB (model2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6acd6e6293c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nN = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainTestFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresults_vectorsizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f7fa10d407fe>\u001b[0m in \u001b[0;36mtrainTestFolder\u001b[0;34m(folder, N)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished loading and preprocessing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrainLpRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLpRDD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_LpRDD\u001b[0m \u001b[0;31m# unpack the RDDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainTestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLpRDD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLpRDD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrainTestFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lemm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f7fa10d407fe>\u001b[0m in \u001b[0;36mtrainTestModel\u001b[0;34m(trainRDD, testRDD)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainTestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Trains 3 models and tests them on training and test data. Returns a matrix with the training and testing (rows) accuracy values for all models (columns). \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainRDD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train models on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# matrix for 2 modes (training/test) vs n models (currently 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-4a8c84b299df>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(lpRDD)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained NB (model2)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print(type(model2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlpRDD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# or this ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained SVM (model3)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[1;32m    555\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    553\u001b[0m             return callMLlibFunc(\"trainSVMModelWithSGD\", rdd, int(iterations), float(step),\n\u001b[1;32m    554\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ll1Ag9BLqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 3000\n",
        "typeFolders = ['bare', 'stop', 'lemm', 'lemm_stop']\n",
        "print('EXPERIMENT 2: Testing different data types')\n",
        "results_preprocessing = []\n",
        "for folder in typeFolders:\n",
        "    print('\\nPath = {}'.format(folder))\n",
        "    result = {'n': n, 't': folder}\n",
        "    result['acc'] = trainTestFolder(folder, n)\n",
        "    results_preprocessing.append(result) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}